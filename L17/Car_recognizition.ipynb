{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('cars_annos.mat')\n",
    "class_names = data['class_names']\n",
    "f_class = open('./label_map1.txt','w')\n",
    "num = 1\n",
    "for j in range(class_names.shape[1]):\n",
    "    class_name = str(class_names[0,j][0]).replace(' ','_')\n",
    "    #print(num,class_name)\n",
    "    f_class.write(str(num) + ' ' + class_name + '\\n')\n",
    "    num = num + 1\n",
    "f_class.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data['annotations']\n",
    "f_train = open('./train.txt','w') \n",
    "for i in range(annotations.shape[1]):    \n",
    "    num = int(annotations[0,i][5])    \n",
    "    num = str(num)       \n",
    "    f_train.write(num+'\\n') \n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119    136\n",
       "79      97\n",
       "161     96\n",
       "167     95\n",
       "56      93\n",
       "      ... \n",
       "175     61\n",
       "64      59\n",
       "158     58\n",
       "99      55\n",
       "136     48\n",
       "Name: 0, Length: 196, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "label = pd.read_csv('train.txt',header = None)\n",
    "label.loc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os \n",
    "import time\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except:\n",
    "        print('Cannot read image:{}'.fromat(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8144\n"
     ]
    }
   ],
   "source": [
    "txt_path = './train.txt'\n",
    "with open(txt_path) as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            #print(lines)\n",
    "            img_labels = list(map(int,[line.strip() for line in lines]))\n",
    "            img_label = img_labels[:8144]\n",
    "            print(len(img_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义数据集类，读取数据和标签\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self,img_path,txt_path,dataset='',data_transforms = None,loader = default_loader):\n",
    "        with open(txt_path) as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            self.img_label = list(map(int,[line.strip() for line in lines]))#读入图片标签\n",
    "        self.img_name = []\n",
    "        for root,dirs,files in os.walk(img_path):            \n",
    "            for name in sorted(files):                \n",
    "                self.img_name.append(os.path.join(img_path,name))\n",
    "        self.data_transforms = data_transforms \n",
    "        self.dataset = dataset\n",
    "        self.loader = loader\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_name)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        img_name = self.img_name[item]\n",
    "        label = self.img_label[item]\n",
    "        img = self.loader(img_name)\n",
    "        \n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "                img = self.data_transforms[self.dataset](img)\n",
    "            except:\n",
    "                print('Cannot transform images:{}'.format(img_name))\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,scheduler,num_epochs,use_gpu):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        begin_time = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        for phase in ['train']:\n",
    "            count_batch = 0\n",
    "            if phase =='train':\n",
    "                scheduler.step()\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            #遍历数据\n",
    "            for data in dataloders[phase]:\n",
    "                count_batch +=1\n",
    "                inputs,labels = data\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    \n",
    "                #梯度归0   \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #前向传播\n",
    "                outputs = model(inputs)\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                loss = criterion(outputs,labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "                \n",
    "                #打印每50次迭代的结果\n",
    "                if count_batch%50 == 0:\n",
    "                    batch_loss = running_loss / (batch_size*count_batch)\n",
    "                    batch_acc = running_corrects / (batch_size*count_batch)\n",
    "                    print('{} Epoch [{}] Batch [{}] Loss: {:.4f} Acc: {:.4f} Time: {:.4f}s'. \\\n",
    "                          format(phase, epoch, count_batch, batch_loss, batch_acc, time.time()-begin_time))\n",
    "                    begin_time = time.time()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            #保存模型\n",
    "            if phase == 'train':\n",
    "                if not os.path.exists('output'):\n",
    "                    os.makedirs('output')\n",
    "                torch.save(model, 'output/resnet_epoch{}.pkl'.format(epoch))\n",
    "                \n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#开始加载数据\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop((256,256),interpolation=3),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])}\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "\n",
    "batch_size = 32\n",
    "num_class = 197\n",
    "\n",
    "image_datasets = {x: Mydataset(img_path='./'+x,\n",
    "                                    txt_path='./'+ x + '.txt',\n",
    "                                    data_transforms=data_transforms,\n",
    "                                    dataset=x) for x in ['train']}\n",
    "\n",
    "#将图片数据和标签放入tensor中\n",
    "dataloders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True) for x in ['train']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x2ec062cd278>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model_ft = models.resnet18(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print(num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Epoch [0] Batch [50] Loss: 0.1689 Acc: 0.0081 Time: 184.1120s\n",
      "train Epoch [0] Batch [100] Loss: 0.1684 Acc: 0.0081 Time: 156.1392s\n",
      "train Epoch [0] Batch [150] Loss: 0.1684 Acc: 0.0088 Time: 152.5495s\n",
      "train Epoch [0] Batch [200] Loss: 0.1683 Acc: 0.0091 Time: 146.8067s\n",
      "train Epoch [0] Batch [250] Loss: 0.1684 Acc: 0.0095 Time: 146.4767s\n",
      "train Loss: 0.1687 Acc: 0.0093\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Epoch [1] Batch [50] Loss: 0.1680 Acc: 0.0106 Time: 139.1851s\n",
      "train Epoch [1] Batch [100] Loss: 0.1683 Acc: 0.0094 Time: 139.6777s\n",
      "train Epoch [1] Batch [150] Loss: 0.1682 Acc: 0.0100 Time: 152.9628s\n",
      "train Epoch [1] Batch [200] Loss: 0.1684 Acc: 0.0089 Time: 173.1314s\n",
      "train Epoch [1] Batch [250] Loss: 0.1684 Acc: 0.0088 Time: 160.2607s\n",
      "train Loss: 0.1687 Acc: 0.0090\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Epoch [2] Batch [50] Loss: 0.1680 Acc: 0.0094 Time: 138.8761s\n",
      "train Epoch [2] Batch [100] Loss: 0.1681 Acc: 0.0094 Time: 137.9469s\n",
      "train Epoch [2] Batch [150] Loss: 0.1682 Acc: 0.0083 Time: 147.2485s\n",
      "train Epoch [2] Batch [200] Loss: 0.1684 Acc: 0.0088 Time: 161.4566s\n",
      "train Epoch [2] Batch [250] Loss: 0.1684 Acc: 0.0091 Time: 162.8285s\n",
      "train Loss: 0.1687 Acc: 0.0091\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Epoch [3] Batch [50] Loss: 0.1690 Acc: 0.0094 Time: 147.5706s\n",
      "train Epoch [3] Batch [100] Loss: 0.1686 Acc: 0.0088 Time: 147.7141s\n",
      "train Epoch [3] Batch [150] Loss: 0.1685 Acc: 0.0083 Time: 142.8524s\n",
      "train Epoch [3] Batch [200] Loss: 0.1683 Acc: 0.0086 Time: 146.6998s\n",
      "train Epoch [3] Batch [250] Loss: 0.1684 Acc: 0.0093 Time: 138.6731s\n",
      "train Loss: 0.1688 Acc: 0.0092\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Epoch [4] Batch [50] Loss: 0.1677 Acc: 0.0106 Time: 153.8492s\n",
      "train Epoch [4] Batch [100] Loss: 0.1684 Acc: 0.0116 Time: 140.6871s\n",
      "train Epoch [4] Batch [150] Loss: 0.1686 Acc: 0.0113 Time: 148.9819s\n",
      "train Epoch [4] Batch [200] Loss: 0.1684 Acc: 0.0103 Time: 139.3578s\n",
      "train Epoch [4] Batch [250] Loss: 0.1684 Acc: 0.0095 Time: 136.9732s\n",
      "train Loss: 0.1688 Acc: 0.0098\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Epoch [5] Batch [50] Loss: 0.1679 Acc: 0.0106 Time: 147.0096s\n",
      "train Epoch [5] Batch [100] Loss: 0.1681 Acc: 0.0106 Time: 136.8700s\n",
      "train Epoch [5] Batch [150] Loss: 0.1682 Acc: 0.0094 Time: 140.9482s\n",
      "train Epoch [5] Batch [200] Loss: 0.1682 Acc: 0.0091 Time: 144.7602s\n",
      "train Epoch [5] Batch [250] Loss: 0.1685 Acc: 0.0091 Time: 136.3611s\n",
      "train Loss: 0.1688 Acc: 0.0090\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Epoch [6] Batch [50] Loss: 0.1681 Acc: 0.0063 Time: 138.0272s\n",
      "train Epoch [6] Batch [100] Loss: 0.1682 Acc: 0.0091 Time: 139.0195s\n",
      "train Epoch [6] Batch [150] Loss: 0.1685 Acc: 0.0100 Time: 136.5680s\n",
      "train Epoch [6] Batch [200] Loss: 0.1684 Acc: 0.0097 Time: 137.2922s\n",
      "train Epoch [6] Batch [250] Loss: 0.1685 Acc: 0.0093 Time: 147.8174s\n",
      "train Loss: 0.1688 Acc: 0.0092\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Epoch [7] Batch [50] Loss: 0.1688 Acc: 0.0119 Time: 144.5674s\n",
      "train Epoch [7] Batch [100] Loss: 0.1685 Acc: 0.0106 Time: 140.4173s\n",
      "train Epoch [7] Batch [150] Loss: 0.1686 Acc: 0.0102 Time: 135.2871s\n",
      "train Epoch [7] Batch [200] Loss: 0.1685 Acc: 0.0098 Time: 142.1198s\n",
      "train Epoch [7] Batch [250] Loss: 0.1685 Acc: 0.0095 Time: 137.2078s\n",
      "train Loss: 0.1687 Acc: 0.0095\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Epoch [8] Batch [50] Loss: 0.1683 Acc: 0.0063 Time: 136.3885s\n",
      "train Epoch [8] Batch [100] Loss: 0.1686 Acc: 0.0075 Time: 137.3943s\n",
      "train Epoch [8] Batch [150] Loss: 0.1685 Acc: 0.0081 Time: 135.3620s\n",
      "train Epoch [8] Batch [200] Loss: 0.1684 Acc: 0.0088 Time: 136.4646s\n",
      "train Epoch [8] Batch [250] Loss: 0.1684 Acc: 0.0086 Time: 150.3269s\n",
      "train Loss: 0.1687 Acc: 0.0087\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Epoch [9] Batch [50] Loss: 0.1681 Acc: 0.0081 Time: 146.8847s\n",
      "train Epoch [9] Batch [100] Loss: 0.1682 Acc: 0.0072 Time: 133.2376s\n",
      "train Epoch [9] Batch [150] Loss: 0.1684 Acc: 0.0077 Time: 137.2432s\n",
      "train Epoch [9] Batch [200] Loss: 0.1685 Acc: 0.0081 Time: 148.6021s\n",
      "train Epoch [9] Batch [250] Loss: 0.1684 Acc: 0.0089 Time: 149.8175s\n",
      "train Loss: 0.1687 Acc: 0.0088\n",
      "Training complete in 123m 20s\n",
      "Best acc: 0.009823\n"
     ]
    }
   ],
   "source": [
    "model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "#model_ft = model_ft.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.2)\n",
    "\n",
    "model_ft = train_model(model=model_ft,\n",
    "                           criterion=criterion,\n",
    "                           optimizer=optimizer_ft,\n",
    "                           scheduler=exp_lr_scheduler,\n",
    "                           num_epochs=10,\n",
    "                           use_gpu=False)\n",
    "\n",
    "torch.save(model_ft,\"output/best_resnet.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
