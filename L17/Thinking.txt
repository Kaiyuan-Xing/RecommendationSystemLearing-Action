1、假设输入是100*100的灰度图像，现在你使用卷积对图像进行特征提取，有50个滤波器，每个卷积核是5*5大小，那么在隐藏层会有多少参数（需要考虑bias参数）
答：输入图片大小，经50个5*5的卷积核过滤后，可以生成50个特征图，每个特征图大小为96*96（100-5+1），神经元的个数为96*96*50=460800个，隐藏层的参数有（5*5+1）*50=1300个。

2、局部不变性和参数共享指的是什么？
答：在图像识别过程中，卷积核对局部区域进行卷积学习得到特定的值，并且在卷积核移动对图片不同位置特征提取时，卷积核的权重参数不发生改变，即卷积过程中进行参数共享 当图片进行平移后，在相同特征的位置，卷积核仍能学习到相同的特定值，因此卷积核具有局部不变性。

3、为什么会用到batch normalization ?
答：神经网络在学习过程中，输入数据经过神经元加权激活以后，输出的数据分布可能发生变化，并作为下一层输入，随着神经网络层数增加，数据的分布不断发生变化，模型在学习过程中需要不断适应数据的变化，从而导致当层数过深时模型的学习速度下降，不能很快收敛，并且神经网络反向传播时由于数据分布差别较大，可能其导数值过小或过大，从而造成梯度消失或爆炸，因此采用BN的方法，将每一层神经网络的输入进行归一化使其保持相同的数据分布，使经过非线性函数激活后的数据分布复合均值为0方差为1的正态分布，从而保证梯度仍然取较大的值，避免了梯度消失的出现，同时由于梯度较大，模型可以采用较大的学习率，大大加快模型的收敛速度，使得模型能够快速收敛。

4、使用dropout可以解决什么问题？
答：深度神经网络采用大量神经元和网络参数，其在训练过程中很容易出现过拟合的现象，导致模型的泛化能力下降，因此在神经网络训练时提出dropout，即在前向传播过程中随机抽取一些神经元不对其进行激活，减少网络中神经元的连接，避免神经网络学习过于依赖一些局部的特征，从而增强模型的泛化能力，避免过拟合的出现。

5、ResNet中的Residual Block解决了什么问题
答：深层CNN在前向传播过程中采用非线性激活函数，从输入到输出进行变换，而在反向传播过程中从输出到输入很难实现完整的反向传播，造成信息丢失，并且随层数增加信息丢失更加严重，从而造成模型训练精度下降。采用残差模块，引入恒等映射X，将输入直接跨层引入到更深层网络中，人为的制造了恒等变换，使整个结构朝着恒等映射的方向收敛，同时简化了优化过程，反向传播时避免了梯度连乘，实现了无损传播梯度，从而避免了梯度消失。另外残差模块可以更好的学习网络中输出与输入之间的变化，从而使得模型学习速度加快。
