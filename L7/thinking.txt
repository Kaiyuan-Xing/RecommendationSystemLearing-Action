1.在CTR点击率预估中，使用GBDT+LR的原理是什么？
答：GBDT+LR是采用stacking方式构建的二分类模型，常用来解决点击率预估问题。点击率预估模型训练样本量大，使用LR线性模型速度较快但过于依赖特征工程，而GBDT算法可以很好的挖掘特征，因此将两个模型的特性结合起来，首先使用GBDT算法训练样本，训练好后将模型预测出的结果进行转换，把模型中的每棵树计算得到的预测概率值所属的叶子结点位置记为1，构造新的训练数据，并将其作为LR模型的输入，进行训练并使用L1正则化防止过拟合，最终完成分类预测。

2.Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）
答：Wide & Deep模型结合线性模型和DNN模型，进行并行训练，在训练过程中同时优化两个模型的参数，并将两个模型的结果的加权作为最终的预测结果进行输出。模型中wide层通过对用户的行为特征进行学习，并采用交叉特征组合挖掘用户与item之间的特征相关性，采用线性模型进行训练，从而对用户行为进行记忆学习，因此具备记忆能力。模型中deep层采用深度神经网络模型，输入连续特征或经过Embeddding后的离散特征，经前馈网络转换为低维稠密向量后传入神经网络进行学习，从而进行更高维度的特征学习，使得模型具有更好的泛化能力。

3.在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？
答：FM与DNN可以采用并行或者串行的方式进行结合：
（1）串行计算主要模型有：
FNN模型：采用FM对数据进行参数初始化实现embedding，将得到数据作为输入传入DNN网络进行训练，实现对高阶组合特征的学习；
NFM模型：对FM进行embedding后的数据直接采用对位相乘后相加起来作为交叉特征，然后传入DNN网络，最后将LR线性部分特征和deep部分特征进行组合，输入最终预测结果。
（2）并行计算主要模型有：
DeepFM模型：将FM和DNN模型采用并行计算，FM学习低阶组合特征，DNN学习高阶组合特征，从而使模型的学习效果提升；
Wide&Deep模型：将LR和DNN模型采用并行计算，wide模型采用人工特征组合解决模型的记忆能力，Deep模型，FNN解决模型的泛化能力，同时训练Wide模型和Deep模型，并将两个模型的结果的加权作为最终的预测结果。

4.Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？
Baseline算法是采用基于统计的基准预测线打分，统计用户评分基线即均值，将其作为预测值，并考虑用户偏差和商品偏差，用户偏差和商品偏差可以通过ALS算法计算得到。
BaselineOnly即只采用baseline算法来对用户评分进行预测
KNNBaseline是采用用户评分均值作为基线，并考虑用户打分的偏差，找到与用户（或商品）相似度最近的K个邻居，并将邻居的行为偏差与相似度加权处理，而邻居用户的行为偏差可以采用baseline算法来获取。

5.GBDT和随机森林都是基于树的算法，它们有什么区别？
GBDT是由多颗CART树构成，采用boosting的方式进行集成，并且在训练过程中下一颗树对上一颗树的残差进行学习，即累加所有树的结果作为最终结果，并且在每一轮迭代中，先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行学习并计算出该弱分类器的权重，最终实现对整个模型的更新。
而随机森林是采用bagging的方式，从训练集中随机抽取n个样本训练生成决策树，然后进行k次抽取，并训练生成k个决策树，将得到的k个模型采用投票的方式得到分类结果或计算均值作为结果，即在训练过程中各个模型重要性相同

6.基于邻域的协同过滤都有哪些算法，请简述原理
UserCF是通过用户的历史行为数据发现用户喜欢的物品，根据不同用户对相同物品的评分或喜欢程度来评测用户之间的相似度，对具有同样爱好的用户进行商品推荐；
itemCF通过不同用户对不同item的行为来评测item之间的相似度，获取与用户之前喜欢商品的相似商品。

